import os
import streamlit as st
import pandas as pd
import random
import torch
import gc


# Otimiza√ß√µes de sistema
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE" 

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from mangaba import Agent 

# --- CONFIGURA√á√ÉO DA P√ÅGINA ---
st.set_page_config(
    page_title="YBY.AI - Sistema Agroecol√≥gico",
    page_icon="üåµ",
    layout="wide"
)

# --- 1. CARREGAMENTO DO MODELO (ENGINE) ---
@st.cache_resource(show_spinner=False)
def load_engine():
    BASE_MODEL = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
    ADAPTER_REPO = "YsraelJS/tinyllama-solo-management-adapters"
    
    container = st.empty()
    container.info("‚öôÔ∏è Carregando C√©rebro Digital YBY (Isso pode demorar na 1¬™ vez)...")
    
    try:
        gc.collect()
        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
        
        # Carregamento otimizado para CPU/Windows
        base_model = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL,
            device_map="cpu", 
            torch_dtype=torch.float32,
            low_cpu_mem_usage=True
        )
        
        model = PeftModel.from_pretrained(base_model, ADAPTER_REPO)
        model = model.merge_and_unload() 
        
        container.empty()
        return tokenizer, model

    except Exception as e:
        container.error(f"‚ö†Ô∏è Modo Offline Ativado (Erro local: {e})")
        return None, None

tokenizer, model = load_engine()
MODE = "IA Local (TinyLlama)" if model else "Modo Nuvem/Simula√ß√£o"

# --- 2. FUN√á√ÉO DE INFER√äNCIA ---
def run_agent(agent: Agent, prompt_text: str, max_tokens=250):
    """
    Gera resposta usando IA Local ou Fallback Simulado.
    """
    if model and tokenizer:
        try:
            system = f"Voc√™ √© {agent.role}. {agent.backstory}. Objetivo: {agent.goal}"
            messages = [
                {"role": "system", "content": system},
                {"role": "user", "content": prompt_text}
            ]
            
            input_ids = tokenizer.apply_chat_template(
                messages, add_generation_prompt=True, return_tensors="pt"
            )
            
            with torch.no_grad():
                outputs = model.generate(
                    input_ids, 
                    max_new_tokens=max_tokens, 
                    do_sample=True, 
                    temperature=0.4, 
                    top_p=0.9
                )
            
            return tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)
        except Exception as e:
            return f"Erro infer√™ncia local: {e}"
    else:
        # FALLBACK INTELIGENTE (SIMULA√á√ÉO)
        # Se o modelo n√£o carregar, geramos respostas baseadas na l√≥gica para n√£o travar a demo
        if "ecol√≥gico" in prompt_text.lower():
            return (
                "**Plano de A√ß√£o Regenerativa (Simulado):**\n\n"
                "1. **Cobertura Morta (Mulching):** Essencial para reter a pouca umidade do semi√°rido e proteger o solo do sol direto.\n"
                "2. **Aduba√ß√£o Org√¢nica:** Incorpore esterco curtido ou compostagem para aumentar a capacidade de reten√ß√£o de √°gua (CRA).\n"
                "3. **Sistema de Gotejamento:** Recomendado para economizar √°gua dado o n√≠vel de umidade atual.\n"
                "4. **Plantio em N√≠vel:** Para evitar eros√£o em chuvas torrenciais."
            )
        else:
            return "Recomenda√ß√£o: **NPK 14-35-14** (Corre√ß√£o de F√≥sforo necess√°ria)."

# --- 3. SIMULADOR IOT ---
st.sidebar.image("https://img.shields.io/badge/YBY.AI-Semi√°rido_Tech-orange", use_container_width=True)
st.sidebar.markdown(f"**Motor:** `{MODE}`")

if 'iot_data' not in st.session_state:
    # Dados padr√£o simulando um dia quente no nordeste
    st.session_state['iot_data'] = {
        'Temperatura': 32.5, 
        'Umidade': 45.0, 
        'Solo_Umid': 28.0, # Solo seco
        'Tipo_Solo': 'Arenoso',
        'Cultura': 'Milho',
        'N': 15, 'P': 8, 'K': 12
    }

if st.sidebar.button("üîÑ Ler Sensores (Tempo Real)"):
    solos = ['Arenoso', 'Argiloso', 'Cascalho', 'Terra Roxa']
    culturas = ['Milho', 'Palma Forrageira', 'Feij√£o Corda', 'Mandioca', 'Algod√£o']
    
    st.session_state['iot_data'] = {
        'Temperatura': round(random.uniform(28.0, 39.0), 1), # Calor
        'Umidade': round(random.uniform(30.0, 60.0), 1),
        'Solo_Umid': round(random.uniform(10.0, 45.0), 1), # Tende a seco
        'Tipo_Solo': random.choice(solos),
        'Cultura': random.choice(culturas),
        'N': random.randint(5, 50),
        'P': random.randint(5, 40),
        'K': random.randint(5, 40)
    }
    st.sidebar.toast("Dados atualizados via Sat√©lite/IoT!", icon="üõ∞Ô∏è")

d = st.session_state['iot_data']

# Exibi√ß√£o Sidebar
c1, c2 = st.sidebar.columns(2)
c1.metric("üå°Ô∏è Temp", f"{d['Temperatura']}¬∞C")
c2.metric("üíß Ar", f"{d['Umidade']}%")
c1.metric("üå± Solo", f"{d['Solo_Umid']}%", delta="-Baixa" if d['Solo_Umid'] < 30 else "Normal")

st.sidebar.divider()
st.sidebar.info(f"Bioma/Solo: **{d['Tipo_Solo']}**")
st.sidebar.warning(f"Cultura: **{d['Cultura']}**")
st.sidebar.markdown("### Nutrientes (NPK)")
cn, cp, ck = st.sidebar.columns(3)
cn.metric("N", d['N'])
cp.metric("P", d['P'])
ck.metric("K", d['K'])

# --- 4. INTERFACE PRINCIPAL ---
st.title("üåµ YBY.AI: Intelig√™ncia Regenerativa")
st.markdown("Plataforma de manejo para solos desafiadores e agricultura de precis√£o.")

tab1, tab2 = st.tabs(["üìä Diagn√≥stico & Plano de A√ß√£o", "üí¨ Consultor YBY"])

# --- ABA 1: RELAT√ìRIO COMPLETO ---
with tab1:
    st.subheader("Diagn√≥stico Integrado")
    
    col_left, col_right = st.columns([1, 1.5])
    
    with col_left:
        st.markdown("#### 1. Corre√ß√£o Qu√≠mica (Imediata)")
        st.caption("Baseado no modelo Fine-Tuned (TinyLlama)")
        
        if st.button("üíä Gerar Recomenda√ß√£o de NPK"):
            with st.spinner("Calculando estequiometria..."):
                agente_quimico = Agent(
                    role="T√©cnico Agr√≠cola",
                    goal="Recomendar fertilizante NPK exato.",
                    backstory="Especialista em tabelas nutricionais.",
                )
                prompt_quimico = (
                    f"Com temperatura {d['Temperatura']}, umidade {d['Umidade']}, "
                    f"solo {d['Tipo_Solo']} para {d['Cultura']}, N={d['N']}, P={d['P']}, K={d['K']}. "
                    f"Qual fertilizante usar?"
                )
                res_quimica = run_agent(agente_quimico, prompt_quimico)
                st.success("Fertilizante Recomendado:")
                st.markdown(f"### {res_quimica}")

    with col_right:
        st.markdown("#### 2. Plano de Manejo Ecol√≥gico (M√©dio Prazo)")
        st.caption("An√°lise regenerativa para solos do semi√°rido/tropicais.")
        
        if st.button("üå≥ Gerar Plano de A√ß√£o Ecol√≥gica"):
            with st.spinner("Consultando base de agroecologia..."):
                
                # AGENTE ECOL√ìGICO (A Novidade)
                agente_eco = Agent(
                    role="Especialista em Agroecologia e Semi√°rido",
                    goal="Criar plano de a√ß√£o para reten√ß√£o de √°gua e vida no solo.",
                    backstory="Voc√™ √© especialista em conviv√™ncia com o semi√°rido. Foco em mat√©ria org√¢nica e √°gua.",
                )
                
                # Prompt enriquecido para for√ßar l√≥gica ecol√≥gica
                prompt_eco = (
                    f"Crie um plano de a√ß√£o curto (3 itens) para tratar um solo do tipo {d['Tipo_Solo']} "
                    f"com umidade cr√≠tica de {d['Solo_Umid']}% e temperatura de {d['Temperatura']}¬∞C. "
                    f"O foco √© a cultura de {d['Cultura']}. "
                    f"Sugira t√©cnicas de reten√ß√£o de √°gua, cobertura de solo e aduba√ß√£o org√¢nica."
                )
                
                res_eco = run_agent(agente_eco, prompt_eco, max_tokens=400)
                
                st.info("Plano de Regenera√ß√£o Sugerido:")
                st.markdown(res_eco)

# --- ABA 2: CHATBOT ---
with tab2:
    st.subheader("Consultor de Campo")
    st.caption("Tire d√∫vidas sobre pragas, sistemas agroflorestais (SAFs) ou manejo.")
    
    chat_agent = Agent(
        role="Assistente YBY", 
        goal="Ajudar o produtor", 
        backstory="Assistente amig√°vel focado em agricultura sustent√°vel."
    )

    if "history" not in st.session_state:
        st.session_state.history = []

    for msg in st.session_state.history:
        with st.chat_message(msg["role"]):
            st.write(msg["content"])

    if prompt := st.chat_input("Ex: Como combater a cochonilha na palma?"):
        st.session_state.history.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.write(prompt)
        
        with st.chat_message("assistant"):
            with st.spinner("Analisando..."):
                resp = run_agent(chat_agent, prompt)
                st.write(resp)
                st.session_state.history.append({"role": "assistant", "content": resp})
